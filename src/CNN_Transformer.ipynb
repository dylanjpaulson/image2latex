{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "from utils import translate_sentence, check_convergence, evaluate, train, plot_losses_bleu\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from data_loader import DataLoader\n",
    "\n",
    "# Create a Vocab object from the vocab list\n",
    "class Vocabulary:\n",
    "    def __init__(self, vocab_list):\n",
    "        self.vocab_list = vocab_list\n",
    "        # Add \" \" to the vocab list\n",
    "        self.vocab_list.append(' ')\n",
    "\n",
    "        # Add special tokens\n",
    "        self.vocab_list.insert(0, '<pad>')\n",
    "        self.vocab_list.insert(1, '<sos>')\n",
    "        self.vocab_list.insert(2, '<eos>')\n",
    "        self.vocab_list.insert(3, '<unk>')\n",
    "\n",
    "        self.id2token = {i: t for i, t in enumerate(self.vocab_list)}\n",
    "        self.token2id = {t: i for i, t in self.id2token.items()}\n",
    "\n",
    "        self.pad_idx = self.token2id['<pad>']\n",
    "        self.sos_idx = self.token2id['<sos>']\n",
    "        self.eos_idx = self.token2id['<eos>']\n",
    "        self.unk_idx = self.token2id['<unk>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_list)\n",
    "    \n",
    "    # Overwrite the indexing operator\n",
    "    def __getitem__(self, index):\n",
    "        return self.vocab_list[index]\n",
    "\n",
    "    def id2string(self, ids):\n",
    "        # Select a subset from self.sos_idx to self.eos_idx\n",
    "        if self.eos_idx in ids:\n",
    "            ids = ids[:ids.index(self.eos_idx)]\n",
    "        if self.sos_idx in ids:\n",
    "            ids = ids[ids.index(self.sos_idx)+1:]\n",
    "\n",
    "        tokens = [self.id2token[id] for id in ids]\n",
    "        return tokens\n",
    "\n",
    "    # Tokenize the formula\n",
    "    def split_string_by_tokens(self, string, max_length):\n",
    "        max_length = max_length - 1\n",
    "        # Sort the vocabulary by length in descending order\n",
    "        vocab = sorted(self.vocab_list, key=len, reverse=True)\n",
    "\n",
    "        # Generate a regular expression pattern that matches any of the vocabulary words\n",
    "        pattern = \"|\".join(re.escape(word) for word in vocab)\n",
    "        \n",
    "        # Use the pattern to split the string into tokens\n",
    "        tokens = re.findall(pattern, string)\n",
    "        \n",
    "        # Add unknown tokens for any characters not matched by the pattern\n",
    "        str_index = 0\n",
    "        updated_tokens = []\n",
    "        i = 0\n",
    "        while i < (len(tokens)):\n",
    "            cur_token = tokens[i]\n",
    "            if string[str_index:str_index+len(cur_token)] == cur_token:\n",
    "                updated_tokens.append(cur_token)\n",
    "                str_index += len(cur_token)\n",
    "                i += 1\n",
    "            else:\n",
    "                updated_tokens.append(f\"<unk>\")\n",
    "                str_index += 1\n",
    "\n",
    "        if str_index != len(string):\n",
    "            for i in range(len(string) - str_index):\n",
    "                updated_tokens.append(f\"<unk>\") \n",
    "\n",
    "        # Append <eos> token\n",
    "        updated_tokens.append('<eos>')\n",
    "\n",
    "        if len(updated_tokens) > max_length:\n",
    "            updated_tokens = updated_tokens[:max_length]\n",
    "        else:\n",
    "            for i in range(max_length - len(updated_tokens)):\n",
    "                updated_tokens.append('<pad>')\n",
    "        \n",
    "        # Insert <sos> token\n",
    "        updated_tokens.insert(0, '<sos>')\n",
    "\n",
    "        # Convert tokens to indices\n",
    "        # indexed_tokens = updated_tokens\n",
    "        indexed_tokens = [self.token2id[token] for token in updated_tokens]\n",
    "\n",
    "        return indexed_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A self-attention module for a transformer-based neural machine translation model.\n",
    "\n",
    "    Args:\n",
    "        embed_size: an integer representing the size of the token embeddings.\n",
    "        heads: an integer representing the number of heads to use in the multi-head attention layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        # set instance variables\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        # make sure embed_size is divisible by heads\n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        # initialize linear layers\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        \"\"\"\n",
    "        Applies self-attention to the input sequences.\n",
    "\n",
    "        Args:\n",
    "            values: a PyTorch tensor of shape (N, value_len, embed_size) representing the values to be used in attention.\n",
    "            keys: a PyTorch tensor of shape (N, key_len, embed_size) representing the keys to be used in attention.\n",
    "            query: a PyTorch tensor of shape (N, query_len, embed_size) representing the queries to be used in attention.\n",
    "            mask: a PyTorch tensor of shape (N, 1, key_len) representing the mask to be applied to the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            A PyTorch tensor of shape (N, query_len, embed_size) representing the output of the self-attention layer.\n",
    "        \"\"\"\n",
    "        # get batch size\n",
    "        N = query.shape[0]\n",
    "\n",
    "        # get sequence lengths\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # split embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim) # (N, value_len, heads, head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)       # (N, key_len, heads, head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim) # (N, query_len, heads, head_dim)\n",
    "\n",
    "        # apply linear transformations\n",
    "        values = self.values(values) # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)       # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(queries) # (N, query_len, heads, head_dim)\n",
    "\n",
    "        # compute attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) # (N, heads, query_len, key_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask out positions with padding tokens\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # apply softmax and normalize by key_len\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) # (N, heads, query_len, key_len)\n",
    "\n",
    "        # compute output using attention scores\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        ) # (N, query_len, heads * head_dim)\n",
    "\n",
    "        # apply final linear transformation\n",
    "        out = self.fc_out(out) # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block with self-attention and feedforward layers.\n",
    "\n",
    "    Args:\n",
    "        embed_size (int): Size of the input and output embeddings.\n",
    "        heads (int): Number of attention heads to use.\n",
    "        dropout (float): Dropout rate to apply.\n",
    "        forward_expansion (int): Size of the hidden layer in the feedforward network.\n",
    "\n",
    "    Inputs:\n",
    "        value (torch.Tensor): Input tensor of shape (N, value_len, embed_size).\n",
    "        key (torch.Tensor): Input tensor of shape (N, key_len, embed_size).\n",
    "        query (torch.Tensor): Input tensor of shape (N, query_len, embed_size).\n",
    "        mask (torch.Tensor): Mask tensor of shape (N, query_len, key_len).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor of shape (N, query_len, embed_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # initialize self-attention, normalization, and feed forward layers\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        # initialize dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for a single Transformer block.\n",
    "\n",
    "        Args:\n",
    "            value (torch.Tensor): Input tensor of shape (N, value_len, embed_size).\n",
    "            key (torch.Tensor): Input tensor of shape (N, key_len, embed_size).\n",
    "            query (torch.Tensor): Input tensor of shape (N, query_len, embed_size).\n",
    "            mask (torch.Tensor): Mask tensor of shape (N, query_len, key_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (N, query_len, embed_size).\n",
    "        \"\"\"\n",
    "        # apply self-attention layer\n",
    "        attention = self.attention(value, key, query, mask) # (N, query_len, embed_size)\n",
    "\n",
    "        # add skip connection, run through normalization and then dropout\n",
    "        x = self.dropout(self.norm1(attention + query)) # (N, query_len, embed_size)\n",
    "\n",
    "        # pass through feed forward network\n",
    "        forward = self.feed_forward(x) # (N, query_len, embed_size)\n",
    "\n",
    "        # add skip connection, run through normalization and then dropout\n",
    "        out = self.dropout(self.norm2(forward + x)) # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN-based encoder that extracts features from input images and adds positional encodings.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int, optional): Number of input channels. Default is 1 for grayscale images.\n",
    "        embed_size (int, optional): Size of the output embeddings. Default is 512.\n",
    "        resnet_type (int, optional): Type of the ResNet model to use. Options are 18, 34, 50, 101, and 152.\n",
    "                                      Default is 34.\n",
    "        max_positional_encoding (int, optional): Maximum length of the positional encoding. Default is 1000.\n",
    "\n",
    "    Inputs:\n",
    "        x (torch.Tensor): Input tensor of shape (N, in_channels, H, W).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor of shape (N, H/32 * W/32, embed_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, embed_size=512, resnet_type=34, max_positional_encoding=1000):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Load the pretrained ResNet model based on the resnet_type parameter\n",
    "        if resnet_type == 18:\n",
    "            resnet = resnet18(pretrained=True)\n",
    "        elif resnet_type == 34:\n",
    "            resnet = resnet34(pretrained=True)\n",
    "        elif resnet_type == 50:\n",
    "            resnet = resnet50(pretrained=True)\n",
    "        elif resnet_type == 101:\n",
    "            resnet = resnet101(pretrained=True)\n",
    "        elif resnet_type == 152:\n",
    "            resnet = resnet152(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resnet_type. Options are 18, 34, 50, 101, and 152.\")\n",
    "\n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        # Remove the last two layers (avgpool and fc)\n",
    "        self.cnn_encoder = nn.Sequential(*list(resnet.children())[:-2])\n",
    "\n",
    "        # Add a 1x1 convolutional layer to adjust the number of channels if needed\n",
    "        if resnet.fc.in_features != embed_size:\n",
    "            self.channel_adjustment = nn.Conv2d(resnet.fc.in_features, embed_size, kernel_size=1)\n",
    "        else:\n",
    "            self.channel_adjustment = None\n",
    "\n",
    "        # Initialize positional encoding\n",
    "        self.register_buffer(\"positional_encoding\", self.get_positional_encoding(max_positional_encoding, embed_size))\n",
    "\n",
    "    def get_positional_encoding(self, max_length, embed_size):\n",
    "        \"\"\"\n",
    "        Generates a positional encoding matrix.\n",
    "\n",
    "        Args:\n",
    "            max_length (int): Maximum length of the positional encoding.\n",
    "            embed_size (int): Size of the embeddings.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding tensor of shape (1, max_length, embed_size).\n",
    "        \"\"\"\n",
    "        # Generate positional encoding\n",
    "        pos_enc = torch.zeros(max_length, embed_size)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.pow(10000, torch.arange(0, embed_size, 2).float() / embed_size)\n",
    "        pos_enc[:, 0::2] = torch.sin(position / div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position / div_term)\n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (N, H/32 * W/32, embed_size).\n",
    "        \"\"\"\n",
    "        # Pass the input image through the CNN encoder\n",
    "        features = self.cnn_encoder(x) # (N, output_channels, H/32, W/32)\n",
    "\n",
    "        # Adjust the number of channels if needed\n",
    "        if self.channel_adjustment is not None:\n",
    "            features = self.channel_adjustment(features)\n",
    "\n",
    "        features = features.view(features.size(0), features.size(1), -1) # (N, output_channels, H/32 * W/32)\n",
    "        features = features.permute(0, 2, 1) # (N, H/32 * W/32, output_channels)\n",
    "\n",
    "        # Add positional encodings\n",
    "        positions = self.positional_encoding[:, :features.size(1), :] # (1, query_len, embed_size)\n",
    "        features = features + positions # (N, query_len, embed_size)\n",
    "\n",
    "        return features # (N, query_len, embed_size)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single decoder block that uses self-attention and a transformer block to process decoder inputs.\n",
    "\n",
    "    Args:\n",
    "        embed_size (int): Size of the input and output embeddings.\n",
    "        heads (int): Number of attention heads to use.\n",
    "        forward_expansion (int): Size of the hidden layer in the feedforward network in the transformer block.\n",
    "        dropout (float): Dropout rate to apply.\n",
    "        device (str): Device to use for computations.\n",
    "\n",
    "    Inputs:\n",
    "        x (torch.Tensor): Input tensor of shape (N, trg_len, embed_size).\n",
    "        value (torch.Tensor): Input tensor of shape (N, src_len, embed_size).\n",
    "        key (torch.Tensor): Input tensor of shape (N, src_len, embed_size).\n",
    "        src_mask (torch.Tensor): Mask tensor of shape (N, 1, src_len).\n",
    "        trg_mask (torch.Tensor): Mask tensor of shape (N, trg_len, trg_len).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor of shape (N, trg_len, embed_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # Initialize self-attention, normalization, and transformer block layers\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "\n",
    "        # Initialize dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, trg_len, embed_size).\n",
    "            value (torch.Tensor): Input tensor of shape (N, src_len, embed_size).\n",
    "            key (torch.Tensor): Input tensor of shape (N, src_len, embed_size).\n",
    "            src_mask (torch.Tensor): Mask tensor of shape (N, 1, src_len).\n",
    "            trg_mask (torch.Tensor): Mask tensor of shape (N, trg_len, trg_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (N, trg_len, embed_size).\n",
    "        \"\"\"\n",
    "        # Self-attention layer\n",
    "        attention = self.attention(x, x, x, trg_mask) # (N, trg_len, embed_size)\n",
    "\n",
    "        # Add skip connection, run through normalization and then dropout\n",
    "        query = self.dropout(self.norm(attention + x)) # (N, trg_len, embed_size)\n",
    "\n",
    "        # Transformer block layer\n",
    "        out = self.transformer_block(value, key, query, src_mask) # (N, trg_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder of the Transformer model that uses multiple decoder blocks to generate the target sequence.\n",
    "\n",
    "    Args:\n",
    "        trg_vocab_size (int): Size of the target vocabulary.\n",
    "        embed_size (int): Size of the input and output embeddings.\n",
    "        num_layers (int): Number of decoder blocks to use.\n",
    "        heads (int): Number of attention heads to use.\n",
    "        forward_expansion (int): Size of the hidden layer in the feedforward network in each decoder block.\n",
    "        dropout (float): Dropout rate to apply.\n",
    "        device (str): Device to use for computations.\n",
    "        max_length (int): Maximum length of the positional encoding.\n",
    "\n",
    "    Inputs:\n",
    "        x (torch.Tensor): Input tensor of shape (N, seq_length).\n",
    "        enc_out (torch.Tensor): Output tensor of the encoder of shape (N, src_seq_length, embed_size).\n",
    "        src_mask (torch.Tensor): Mask tensor for the source sequence of shape (N, 1, src_seq_length).\n",
    "        trg_mask (torch.Tensor): Mask tensor for the target sequence of shape (N, seq_length, seq_length).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor of shape (N, seq_length, trg_vocab_size).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length\n",
    "        ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Initialize word embedding layer\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "\n",
    "        # Initialize decoder blocks\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Initialize output linear layer\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "\n",
    "        # Initialize dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize positional encoding\n",
    "        self.register_buffer(\"positional_encoding\", self.get_positional_encoding(max_length, embed_size))\n",
    "    \n",
    "    def get_positional_encoding(self, max_length, embed_size):\n",
    "        \"\"\"\n",
    "        Generates a positional encoding matrix.\n",
    "\n",
    "        Args:\n",
    "            max_length (int): Maximum length of the positional encoding.\n",
    "            embed_size (int): Size of the embeddings.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding tensor of shape (1, max_length, embed_size).\n",
    "        \"\"\"\n",
    "        # Generate positional encoding\n",
    "        pos_enc = torch.zeros(max_length, embed_size)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.pow(10000, torch.arange(0, embed_size, 2).float() / embed_size)\n",
    "        pos_enc[:, 0::2] = torch.sin(position / div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position / div_term)\n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, seq_length).\n",
    "            enc_out (torch.Tensor): Output tensor of the encoder of shape (N, src_seq_length, embed_size).\n",
    "            src_mask (torch.Tensor): Mask tensor for the source sequence of shape (N, 1, src_seq_length).\n",
    "            trg_mask (torch.Tensor): Mask tensor for the target sequence of shape (N, seq_length, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (N, seq_length, trg_vocab_size).\n",
    "        \"\"\"\n",
    "        N, seq_length = x.shape\n",
    "\n",
    "        # Add word embeddings\n",
    "        x = self.word_embedding(x) # (N, seq_length, embed_size)\n",
    "\n",
    "        # Add positional encodings\n",
    "        positions = self.positional_encoding[:, :seq_length, :] # (1, seq_length, embed_size)\n",
    "        x = self.dropout(x + positions) # (N, seq_length, embed_size)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask) # (N, seq_length, embed_size)\n",
    "\n",
    "        # Pass through output linear layer\n",
    "        out = self.fc_out(x) # (N, seq_length, trg_vocab_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The Transformer model that uses an encoder and decoder to generate the target sequence.\n",
    "\n",
    "    Args:\n",
    "        trg_vocab_size (int): Size of the target vocabulary.\n",
    "        trg_pad_idx (int): Index of the target padding token.\n",
    "        embed_size (int): Size of the input and output embeddings.\n",
    "        resnet_type (int): Type of ResNet to use for the encoder.\n",
    "        num_layers (int): Number of decoder blocks to use.\n",
    "        forward_expansion (int): Size of the hidden layer in the feedforward network in each decoder block.\n",
    "        heads (int): Number of attention heads to use.\n",
    "        dropout (float): Dropout rate to apply.\n",
    "        device (str): Device to use for computations.\n",
    "        max_length (int): Maximum length of the positional encoding.\n",
    "\n",
    "    Inputs:\n",
    "        src (torch.Tensor): Input tensor of shape (N, src_seq_length).\n",
    "        trg (torch.Tensor): Target tensor of shape (N, trg_seq_length).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor of shape (N, trg_seq_length, trg_vocab_size).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            trg_vocab_size,\n",
    "            trg_pad_idx,\n",
    "            embed_size=512,\n",
    "            resnet_type=34,\n",
    "            num_layers=6,\n",
    "            forward_expansion=4,\n",
    "            heads=8,\n",
    "            dropout=0,\n",
    "            device=\"cuda\",\n",
    "            max_length=200\n",
    "        ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Initialize hyperparameters\n",
    "        self.embed_size = embed_size\n",
    "        self.resnet_type = resnet_type\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_expansion = forward_expansion\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Initialize encoder and decoder\n",
    "        self.encoder = Encoder(\n",
    "            in_channels=1,\n",
    "            embed_size=embed_size,\n",
    "            resnet_type=resnet_type,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        # Store padding index and device\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Creates a target mask for the decoder.\n",
    "\n",
    "        Args:\n",
    "            trg (torch.Tensor): Target caption tensor of shape (N, trg_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Target mask tensor of shape (N, 1, trg_len, trg_len).\n",
    "        \"\"\"\n",
    "        # Create target mask\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ) # (N, 1, trg_len, trg_len)\n",
    "        return trg_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer model.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source image tensor of shape (N, C, H, W).\n",
    "            trg (torch.Tensor): Target caption tensor of shape (N, trg_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted caption tensor of shape (N, trg_len, trg_vocab_size).\n",
    "        \"\"\"\n",
    "        # Create masks\n",
    "        src_mask = None\n",
    "        trg_mask = self.make_trg_mask(trg) # (N, 1, trg_len, trg_len)\n",
    "\n",
    "        # Pass through encoder\n",
    "        enc_src = self.encoder(src) # (N, query_len, embed_size)\n",
    "\n",
    "        # Pass through decoder\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask) # (N, trg_len, trg_vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 533\n",
      "Number of images: 82828\n",
      "Number of im2formula mappings: 82828\n",
      "Number of images: 10353\n",
      "Number of im2formula mappings: 10353\n",
      "Number of images: 10355\n",
      "Number of im2formula mappings: 10355\n",
      "60ee748793.png\n",
      "1cbb05a562.png\n",
      "ed164cc822.png\n",
      "e265f9dc6b.png\n",
      "72f6bc494a.png\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Open vocab.txt and load each line into a list, remove new line character\n",
    "with open('processed_data/vocab.txt', 'r', encoding='windows-1252') as f:\n",
    "    vocab_list = [line.strip() for line in f.readlines()]\n",
    "latex_vocab = Vocabulary(vocab_list)\n",
    "print('Vocab size: {}'.format(len(latex_vocab.vocab_list)))\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 16\n",
    "vocab_size = len(vocab_list)\n",
    "max_token_length = 200\n",
    "\n",
    "# Create a transform for binary images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load training, validation, and test data\n",
    "train_data = DataLoader('processed_data/train_all', 'processed_data/train_formulas_all.txt', latex_vocab, transform, max_token_length)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "validation_data = DataLoader('processed_data/validation_all', 'processed_data/validation_formulas_all.txt', latex_vocab, transform, max_token_length)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_data = DataLoader('processed_data/test_all', 'processed_data/test_formulas_all.txt', latex_vocab, transform, max_token_length)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Print file names for the first 5 samples in train_data\n",
    "for i in range(5):\n",
    "    image, formula_idx, fname = train_data[i]\n",
    "    print(fname)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Loader Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 82828\n",
      "308e3ebcf5.png\n",
      "Label: G_{ab}^{(1)} = \\left\\{ \\begin{array}{ll} 0 & \\mbox{for $a,b\\ne i,j$} \\\\ f_if_j+k_{ij}+\\delta_{ij}A\\rho & \\mbox{for $a,b= i,j$} \\end{array}\\right.\n",
      "Token Idxs: [1, 56, 495, 528, 497, 498, 530, 494, 528, 8, 30, 9, 530, 532, 46, 532, 272, 532, 146, 528, 510, 510, 530, 532, 24, 532, 6, 532, 291, 498, 514, 525, 528, 503, 514, 518, 532, 19, 497, 12, 498, 318, 532, 506, 12, 508, 19, 530, 532, 128, 532, 503, 495, 506, 503, 495, 508, 11, 509, 495, 528, 506, 508, 530, 11, 194, 495, 528, 506, 508, 530, 50, 380, 532, 6, 532, 291, 498, 514, 525, 528, 503, 514, 518, 532, 19, 497, 12, 498, 46, 532, 506, 12, 508, 19, 530, 532, 206, 382, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokens: ['<sos>', 'G', '_', '{', 'a', 'b', '}', '^', '{', '(', '1', ')', '}', ' ', '=', ' ', '\\\\left\\\\{', ' ', '\\\\begin{array}', '{', 'l', 'l', '}', ' ', '0', ' ', '&', ' ', '\\\\m', 'b', 'o', 'x', '{', 'f', 'o', 'r', ' ', '$', 'a', ',', 'b', '\\\\ne', ' ', 'i', ',', 'j', '$', '}', ' ', '\\\\\\\\', ' ', 'f', '_', 'i', 'f', '_', 'j', '+', 'k', '_', '{', 'i', 'j', '}', '+', '\\\\delta', '_', '{', 'i', 'j', '}', 'A', '\\\\rho', ' ', '&', ' ', '\\\\m', 'b', 'o', 'x', '{', 'f', 'o', 'r', ' ', '$', 'a', ',', 'b', '=', ' ', 'i', ',', 'j', '$', '}', ' ', '\\\\end{array}', '\\\\right.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "200\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1254x550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 10353\n",
      "7623c5a3e2.png\n",
      "Label: u_{xx}+u_{yy}+\\left[\\left(1+{u\\over 4}\\right)^3\\right]_{zz}=0.\n",
      "Token Idxs: [1, 521, 495, 528, 525, 525, 530, 11, 521, 495, 528, 526, 526, 530, 11, 266, 262, 30, 11, 528, 521, 343, 532, 36, 530, 381, 494, 33, 391, 495, 528, 527, 527, 530, 46, 24, 16, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokens: ['<sos>', 'u', '_', '{', 'x', 'x', '}', '+', 'u', '_', '{', 'y', 'y', '}', '+', '\\\\left[', '\\\\left(', '1', '+', '{', 'u', '\\\\over', ' ', '4', '}', '\\\\right)', '^', '3', '\\\\right]', '_', '{', 'z', 'z', '}', '=', '0', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "200\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1254x550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 10355\n",
      "64d42f6c28.png\n",
      "Label: g(p_1,p_2,p_3;l)=g(0)f(p_1,p_2,p_3;l)= e f(p_1,p_2,p_3;l)\\,,\n",
      "Token Idxs: [1, 504, 8, 515, 495, 30, 12, 515, 495, 32, 12, 515, 495, 33, 44, 510, 9, 46, 504, 8, 24, 9, 503, 8, 515, 495, 30, 12, 515, 495, 32, 12, 515, 495, 33, 44, 510, 9, 46, 532, 502, 532, 503, 8, 515, 495, 30, 12, 515, 495, 32, 12, 515, 495, 33, 44, 510, 9, 85, 12, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokens: ['<sos>', 'g', '(', 'p', '_', '1', ',', 'p', '_', '2', ',', 'p', '_', '3', ';', 'l', ')', '=', 'g', '(', '0', ')', 'f', '(', 'p', '_', '1', ',', 'p', '_', '2', ',', 'p', '_', '3', ';', 'l', ')', '=', ' ', 'e', ' ', 'f', '(', 'p', '_', '1', ',', 'p', '_', '2', ',', 'p', '_', '3', ';', 'l', ')', '\\\\,', ',', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "200\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1254x550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "datasets = [train_data, validation_data, test_data]\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Print length of dataset\n",
    "    print('Length of dataset: {}'.format(len(dataset)))\n",
    "\n",
    "    # Get a random number from len(train_data)\n",
    "    rand_num = np.random.randint(0, len(train_data))\n",
    "\n",
    "    # Get the image and label from the dataset\n",
    "    image, label, file_name = train_data[rand_num]\n",
    "\n",
    "    print(file_name)\n",
    "\n",
    "    # Convert the tensor to a PIL image\n",
    "    image_pil = transforms.ToPILImage()(image)\n",
    "\n",
    "    # Print the label\n",
    "    print('Label: {}'.format(label))\n",
    "\n",
    "    # Print the label as tokens\n",
    "    tokens = latex_vocab.split_string_by_tokens(label, max_token_length)\n",
    "    print('Token Idxs: {}'.format(tokens))\n",
    "    print('Tokens: {}'.format([latex_vocab.id2token[id] for id in tokens]))\n",
    "    print(len(tokens))\n",
    "\n",
    "    # Show the image in Jupyter notebook\n",
    "    display(image_pil)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 / 6]\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "model = Transformer(\n",
    "    trg_vocab_size=len(latex_vocab),\n",
    "    trg_pad_idx=latex_vocab.pad_idx,\n",
    "    embed_size=1024,\n",
    "    resnet_type=50,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=4,\n",
    "    dropout=0,\n",
    "    device=device,\n",
    "    max_length=200  \n",
    ").to(device)\n",
    "\n",
    "# Train the model\n",
    "train_loss, blue_scores = train(model=model, \n",
    "                                model_name='im2latex_all_ns', \n",
    "                                train_loader=train_loader, \n",
    "                                validation_loader=validation_loader,\n",
    "                                vocab_obj=latex_vocab,\n",
    "                                max_token_length=200,\n",
    "                                num_epochs=10,\n",
    "                                learning_rate=1e-4, \n",
    "                                device=device,\n",
    "                                print_every=40)\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plot_losses_bleu(train_loss, blue_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer\n",
    "import dill\n",
    "\n",
    "# Define the function that trains and evaluates the model\n",
    "def train_and_evaluate(hyperparams):\n",
    "    resnet_type, num_layers, embed_size, heads = hyperparams\n",
    "    print(\"Training with hyperparameter:\")\n",
    "    print(\"Resnet Type: {}\".format(resnet_type))\n",
    "    print(\"Number of layers: {}\".format(num_layers))\n",
    "    print(\"Embedding size: {}\".format(embed_size))\n",
    "    print(\"Number of heads: {}\".format(heads))\n",
    "    print()\n",
    "\n",
    "    model_name = \"resnetType{}_layers{}_embed{}_heads{}\".format(resnet_type, num_layers, embed_size, heads)\n",
    "\n",
    "    # Modify the model definition to use the provided hyperparameters\n",
    "    model = Transformer(\n",
    "        trg_vocab_size=len(latex_vocab),\n",
    "        trg_pad_idx=latex_vocab.pad_idx,\n",
    "        embed_size=embed_size,\n",
    "        resnet_type=resnet_type,\n",
    "        num_layers=num_layers,\n",
    "        forward_expansion=4,\n",
    "        heads=heads,\n",
    "        dropout=0,\n",
    "        device=device,\n",
    "        max_length=200  \n",
    "    ).to(device)\n",
    "\n",
    "    train(model=model, \n",
    "          model_name=model_name, \n",
    "          train_loader=train_loader, \n",
    "          validation_loader=validation_loader,\n",
    "          vocab_obj=latex_vocab,\n",
    "          max_token_length=200,\n",
    "          num_epochs=3,\n",
    "          learning_rate=1e-4, \n",
    "          device=device,\n",
    "          print_every=40)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    print(\"Evaluating the model on the test set...\")\n",
    "    bleu_score = evaluate(model, test_loader, latex_vocab, device, max_length=200)\n",
    "    print(\"BLEU Score: {}\".format(bleu_score))\n",
    "\n",
    "    # Open the log file and write the evaluation results\n",
    "    log_filename = model_name + \"_log.txt\"\n",
    "    log_path = \"./models/\" + model_name + \"/\" + log_filename\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(\"Model Name: {}\\n\".format(model_name))\n",
    "        f.write(\"Evaluation on the test set:\\n\")\n",
    "        f.write(\"BLEU Score: {}\\n\".format(bleu_score))\n",
    "\n",
    "    # Return the negative BLEU score (since the optimizer tries to minimize the objective)\n",
    "    return -bleu_score\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = [\n",
    "    Categorical([18, 34, 50], name='resnet_type'),\n",
    "    Categorical([1, 3, 6], name='num_layers'),\n",
    "    Categorical([256, 512, 1024, 2048], name='embed_size'),\n",
    "    Categorical([4, 8, 16], name='heads'),\n",
    "]\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "from skopt import gp_minimize\n",
    "\n",
    "# Define a callback function that saves the optimization state after each iteration\n",
    "def save_result(result, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        dill.dump(result, f)\n",
    "\n",
    "# Initialize the optimization object with some initial random samples\n",
    "result = gp_minimize(\n",
    "    func=train_and_evaluate,\n",
    "    dimensions=search_space,\n",
    "    n_calls=50,\n",
    "    random_state=42,\n",
    "    callback=lambda res: save_result(res, 'optimization_state.pkl')\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparams = result.x\n",
    "print(\"Best hyperparameters: \", best_hyperparams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1254x550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Formula: \n",
      "G_{\\mu\\nu}= F_{\\mu\\nu}^{corr} ( R_{\\mu\\nu\\lambda\\sigma},D_{\\delta} R_{\\mu\\nu\\lambda\\sigma}, \\dots)\n",
      "\n",
      "Predicted Formula 1: \n",
      "G_{\\mu\\nu} = F^{corr}_{\\mu\\nu} (R_{\\mu\\nu\\lambda\\sigma},D_\\delta R_{\\mu\\nu\\lambda\\sigma},\\ldots)\n",
      "BLEU Score: 0.6905410100184718\n",
      "Output Length: 48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Check for CUDA availability and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Model hyperparameters and initialization\n",
    "model = Transformer(\n",
    "    trg_vocab_size=len(latex_vocab),\n",
    "    trg_pad_idx=latex_vocab.pad_idx,\n",
    "    embed_size=1024,\n",
    "    resnet_type=50,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=4,\n",
    "    dropout=0,\n",
    "    device=device,\n",
    "    max_length=200  \n",
    ").to(device)\n",
    "\n",
    "# Load pre-trained model\n",
    "model.load_state_dict(torch.load('./models/im2latex_all/im2latex_all.pkl', \n",
    "                                 map_location=torch.device(device)))\n",
    "\n",
    "# Load random image from the test set\n",
    "all_images = os.listdir('processed_data/test_all')\n",
    "image_name = all_images[np.random.randint(0, len(all_images))]\n",
    "example_image = Image.open('processed_data/test_all/' + image_name)\n",
    "\n",
    "# Invert the image\n",
    "inverted_image = ImageOps.invert(example_image)\n",
    "\n",
    "# Display the inverted image\n",
    "display(inverted_image)\n",
    "\n",
    "# Find the corresponding formula for the selected image\n",
    "with open('processed_data/test_formulas_all.txt', 'r', encoding='windows-1252') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ')\n",
    "        if (parts[0] == image_name):\n",
    "            ref = ' '.join(parts[1:])\n",
    "            print(\"Target Formula: \")\n",
    "            print(' '.join(parts[1:]))\n",
    "\n",
    "# Evaluate the model and generate predictions\n",
    "model.eval()\n",
    "translated_sentence = translate_sentence(\n",
    "    model, example_image, latex_vocab, transform, device, k=1, max_length=200\n",
    ")\n",
    "\n",
    "# Print predicted formulas and calculate their BLEU scores\n",
    "for i in range(len(translated_sentence)):\n",
    "    print(\"\\nPredicted Formula {}: \".format(i+1))\n",
    "    print(\"\".join(translated_sentence[i]))\n",
    "    # print(\" \".join(translated_sentence[i]), \"\\n\")\n",
    "\n",
    "    token_idxs = latex_vocab.split_string_by_tokens(ref, max_length=200)\n",
    "    tokens = [latex_vocab.id2string(idx) for idx in [token_idxs]]\n",
    "    print(\"BLEU Score: {}\".format(sentence_bleu([translated_sentence[i]], tokens[0])))\n",
    "    print(\"Output Length: {}\".format(len(translated_sentence[i])))\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Model hyperparameters\n",
    "model = Transformer(\n",
    "    trg_vocab_size=len(latex_vocab),\n",
    "    trg_pad_idx=latex_vocab.pad_idx,\n",
    "    embed_size=1024,\n",
    "    resnet_type=50,\n",
    "    num_layers=6,\n",
    "    forward_expansion=4,\n",
    "    heads=4,\n",
    "    dropout=0,\n",
    "    device=device,\n",
    "    max_length=200  \n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('.models\\im2latex_all\\im2latex_all.pkl', \n",
    "                                 map_location=torch.device(device)))\n",
    "# Evaluate the model on the validation set\n",
    "print(\"Evaluating the model on the test set...\")\n",
    "bleu_score = evaluate(model, test_loader, latex_vocab, device, max_length=200)\n",
    "print(\"BLEU Score: {}\".format(bleu_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
